{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGj9jjk0pKxs"
      },
      "outputs": [],
      "source": [
        "#@title License information { display-mode: \"form\" }\n",
        "#@markdown Copyright 2024 The MediaPipe Authors.\n",
        "#@markdown Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#@markdown\n",
        "#@markdown you may not use this file except in compliance with the License.\n",
        "#@markdown You may obtain a copy of the License at\n",
        "#@markdown\n",
        "#@markdown https://www.apache.org/licenses/LICENSE-2.0\n",
        "#@markdown\n",
        "#@markdown Unless required by applicable law or agreed to in writing, software\n",
        "#@markdown distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#@markdown WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#@markdown See the License for the specific language governing permissions and\n",
        "#@markdown limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup { display-mode: \"form\" }\n",
        "!pip install --upgrade numpy\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "install_out = widgets.Output()\n",
        "display(install_out)\n",
        "with install_out:\n",
        "  !pip install --upgrade numpy\n",
        "  !pip install --upgrade scipy\n",
        "  !pip install mediapipe\n",
        "  from mediapipe.tasks.python.genai import bundler\n",
        "\n",
        "install_out.clear_output()\n",
        "with install_out:\n",
        "  print(\"Setup done.\")"
      ],
      "metadata": {
        "id": "owxiD-Pxp26x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "tokenizer.save_pretrained(\"bert_tokenizer/\")\n"
      ],
      "metadata": {
        "id": "-WNDVBDFAbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build_fake_corpus.py\n",
        "with open(\"bert_tokenizer/vocab.txt\", \"r\") as fin, open(\"fake_corpus.txt\", \"w\") as fout:\n",
        "    for line in fin:\n",
        "        token = line.strip()\n",
        "        if token and not token.startswith(\"[\"):\n",
        "            fout.write(token.replace(\"##\", \"\") + \"\\n\")\n"
      ],
      "metadata": {
        "id": "e6ygI-8tIyTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "waUArrZwOW22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-keras # Install the compatible Keras for transformers\n",
        "!pip install --upgrade tensorflow # Ensure TensorFlow is upgraded\n",
        "!pip install tensorflow --no-deps # Reinstall TensorFlow without dependencies\n",
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "import tensorflow as tf # Import tensorflow\n",
        "\n",
        "# Instead of importing from tf_keras, import from tensorflow.keras.activations\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model.save(\"distilbert_tf_model\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"distilbert_tf_model\") # Use tf.lite\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"distilbert-qa.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "E5Uv84i_5RLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"spm_train\" 2>/dev/null"
      ],
      "metadata": {
        "id": "9_8iADI8SqQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y cmake build-essential pkg-config libgoogle-perftools-dev\n",
        "!git clone https://github.com/google/sentencepiece.git\n",
        "%cd sentencepiece\n",
        "!mkdir build && cd build && cmake .. && make -j $(nproc) && make install && ldconfig\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "oZFX2ylW3WYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spm_train --version"
      ],
      "metadata": {
        "id": "OYDF8dsV4l8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spm_train \\\n",
        "      --input=fake_corpus.txt \\\n",
        "      --model_prefix=distilbert_tokenizer \\\n",
        "      --vocab_size=26598 \\\n",
        "      --character_coverage=1.0 \\\n",
        "      --model_type=word"
      ],
      "metadata": {
        "id": "jeHbp4-O4oft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --force-reinstall mediapipe-model-maker==0.2.1.4\n",
        "#!pip install --upgrade  --user mediapipe-model-maker==0.2.1.4\n",
        "!python3 -m mediapipe_model_maker.genai.task_converter \\\n",
        "  --tflite_model distilbert-qa.tflite \\\n",
        "  --tokenizer_model distilbert_tokenizer.model \\\n",
        "  --output_file distilbert_qa.task"
      ],
      "metadata": {
        "id": "aceRhGxH5XSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe\n",
        "!pip install --upgrade mediapipe-model-maker # Install or upgrade mediapipe-model-maker\n",
        "!pip install sentencepiece  # Install or upgrade sentencepiece\n"
      ],
      "metadata": {
        "id": "CLu6ITbxyYkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow text\n",
        "#!pip install tensorflow_hub\n",
        "#!pip uninstall mediapipe-model-maker\n",
        "#!pip install --upgrade mediapipe-model-maker\n",
        "\n",
        "#!pip uninstall mediapipe-model-maker\n",
        "#!pip install --upgrade --force-reinstall mediapipe-model-maker==0.2.1.4\n",
        "\n",
        "!find / -name \"mediapipe_model_maker\" 2>/dev/null\n",
        "!find / -name \"model_maker\" 2>/dev/null\n",
        "\n",
        "\n",
        "import site\n",
        "import os\n",
        "\n",
        "for path in site.getsitepackages():\n",
        "    package_path = os.path.join(path, 'mediapipe_model_maker')\n",
        "    if os.path.exists(package_path):\n",
        "        print(f\"Found mediapipe_model_maker at: {package_path}\")\n",
        "        break  # Stop searching once found\n",
        "else:\n",
        "    print(\"mediapipe_model_maker not found in site-packages.\")\n",
        "\n",
        "\n",
        "!ls {package_path}\n",
        "!ls {package_path}/genai  # Assuming genai subfolder exists"
      ],
      "metadata": {
        "id": "_fikOg2Eyjl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zpnGQA_AzUNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf; print(tf.__version__)\n",
        "\n",
        "!ls /usr/local/lib/python3.11/dist-packages/mediapipe_model_maker\n",
        "!ls /usr/local/lib/python3.11/dist-packages/mediapipe_model_maker/genai"
      ],
      "metadata": {
        "id": "ES12ZzvWzUsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why do we need task bundles?\n",
        "\n",
        "Executing a text generation pipeline in its entirety requires more than merely employing the core transformer model. It requires preparing the input text to align with the model's required format, running the model autoregressively, and sampling during each iteration. Consequently, once the user has converted their model into `tflite` format (comprising the model's graph and parameters), it becomes essential to augment it with additional metadata to ensure successful end-to-end execution of the model.\n",
        "\n",
        "Task bundler offers a practical approach to creating such bundles. The example below illustrates how a task bundle can be generated for a converted Gemma model."
      ],
      "metadata": {
        "id": "5Z6c-cpvqd25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mediapipe.tasks.python.genai import bundler\n",
        "\n",
        "tflite_model=\"distilbert-qa.tflite\" # @param {type:\"string\"}\n",
        "tokenizer_model=\"distilbert_tokenizer.model\" # @param {type:\"string\"}\n",
        "start_token=\"<bos>\" # @param {type:\"string\"}\n",
        "stop_token=\"<eos>\" # @param {type:\"string\"}\n",
        "output_filename=\"distilbert.task\" # @param {type:\"string\"}\n",
        "enable_bytes_to_unicode_mapping=False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "config = bundler.BundleConfig(\n",
        "    tflite_model=tflite_model,\n",
        "    tokenizer_model=tokenizer_model,\n",
        "    start_token=start_token,\n",
        "    stop_tokens=[stop_token],\n",
        "    output_filename=output_filename,\n",
        "    enable_bytes_to_unicode_mapping=enable_bytes_to_unicode_mapping,\n",
        ")\n",
        "bundler.create_bundle(config)"
      ],
      "metadata": {
        "id": "Cs4KJzSzqb7y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IliBF1FVRx4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the Bundle { display-mode: \"form\" }\n",
        "#@markdown Run this cell to download the generated `.task` file.\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "FnFkpfr1bwtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "\n",
        "* The current task pipeline only supports SentencePiece tokenizer models.\n",
        "* Certain models (e.g., phi-2) use bytes to unicode mapping. Use `enable_bytes_to_unicode_mapping` flag accordingly. Such information, including `start_token` and `stop_tokens` are often provided along with model artifacts.\n",
        "* The generated output bundle file must end with `.task`. If not, `create_bundle` automatically adds the extension. Do not remove or change that."
      ],
      "metadata": {
        "id": "_Unaye8xsfio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "model_path = \"distilbert-qa.tflite\"  # Replace with your model\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input Tensor Details:\", input_details)\n",
        "print(\"Output Tensor Details:\", output_details)\n",
        "\n",
        "# Assuming your model has two inputs: input_ids and attention_mask\n",
        "# Adjust the input shapes and data types accordingly\n",
        "input_shape_ids = input_details[0]['shape']\n",
        "input_data_ids = np.random.randint(0, 30522, size=input_shape_ids).astype(np.int32) # Generate random token IDs\n",
        "\n",
        "input_shape_mask = input_details[1]['shape']\n",
        "input_data_mask = np.ones(input_shape_mask).astype(np.int32) # Create an attention mask of all 1s\n",
        "\n",
        "# Set the tensors for both inputs\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data_ids)\n",
        "interpreter.set_tensor(input_details[1]['index'], input_data_mask)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "output_index = output_details[0]['index']\n",
        "output_data = interpreter.get_tensor(output_index)\n",
        "\n",
        "print(\"Raw Output Data:\", output_data)\n",
        "\n",
        "# Add your post-processing here (if needed)\n",
        "# ..."
      ],
      "metadata": {
        "id": "xzaJD5Rz3lwB",
        "outputId": "a2ffa938-fa99-4fe3-ea00-171cf96da2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor Details: [{'name': 'serving_default_attention_mask:0', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_input_ids:0', 'index': 1, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output Tensor Details: [{'name': 'StatefulPartitionedCall:0', 'index': 724, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 723, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Raw Output Data: [[-6.622172]]\n"
          ]
        }
      ]
    }
  ]
}