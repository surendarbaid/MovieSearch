{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6bf3fef6888648ec9bfb849fdaea7d90": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3ea965eb751649d5bde24d5393e84a61",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Setup done.\n"
                ]
              }
            ]
          }
        },
        "3ea965eb751649d5bde24d5393e84a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pGj9jjk0pKxs"
      },
      "outputs": [],
      "source": [
        "#@title License information { display-mode: \"form\" }\n",
        "#@markdown Copyright 2024 The MediaPipe Authors.\n",
        "#@markdown Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#@markdown\n",
        "#@markdown you may not use this file except in compliance with the License.\n",
        "#@markdown You may obtain a copy of the License at\n",
        "#@markdown\n",
        "#@markdown https://www.apache.org/licenses/LICENSE-2.0\n",
        "#@markdown\n",
        "#@markdown Unless required by applicable law or agreed to in writing, software\n",
        "#@markdown distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#@markdown WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#@markdown See the License for the specific language governing permissions and\n",
        "#@markdown limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup { display-mode: \"form\" }\n",
        "!pip install --upgrade numpy\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "install_out = widgets.Output()\n",
        "display(install_out)\n",
        "with install_out:\n",
        "  !pip install mediapipe\n",
        "  from mediapipe.tasks.python.genai import bundler\n",
        "\n",
        "install_out.clear_output()\n",
        "with install_out:\n",
        "  print(\"Setup done.\")"
      ],
      "metadata": {
        "id": "owxiD-Pxp26x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428,
          "referenced_widgets": [
            "6bf3fef6888648ec9bfb849fdaea7d90",
            "3ea965eb751649d5bde24d5393e84a61"
          ]
        },
        "outputId": "f3aea623-26d2-4f36-b0da-1b2bbc338e8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.1 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.4 which is incompatible.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow-model-optimization 0.7.5 requires numpy~=1.23, but you have numpy 2.2.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "flax 0.10.5 requires jax>=0.5.1, but you have jax 0.4.34 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.1 which is incompatible.\n",
            "orbax-checkpoint 0.11.10 requires jax>=0.5.0, but you have jax 0.4.34 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bf3fef6888648ec9bfb849fdaea7d90"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "tokenizer.save_pretrained(\"bert_tokenizer/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WNDVBDFAbQs",
        "outputId": "aadfd593-427d-41c6-c3b1-c0780d81d39c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bert_tokenizer/tokenizer_config.json',\n",
              " 'bert_tokenizer/special_tokens_map.json',\n",
              " 'bert_tokenizer/vocab.txt',\n",
              " 'bert_tokenizer/added_tokens.json',\n",
              " 'bert_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build_fake_corpus.py\n",
        "with open(\"bert_tokenizer/vocab.txt\", \"r\") as fin, open(\"fake_corpus.txt\", \"w\") as fout:\n",
        "    for line in fin:\n",
        "        token = line.strip()\n",
        "        if token and not token.startswith(\"[\"):\n",
        "            fout.write(token.replace(\"##\", \"\") + \"\\n\")\n"
      ],
      "metadata": {
        "id": "e6ygI-8tIyTc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "waUArrZwOW22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7f018b-19f9-4a81-b017-0205258d44fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: fake_corpus.txt\n",
            "  input_format: \n",
            "  model_prefix: distilbert_tokenizer\n",
            "  model_type: WORD\n",
            "  vocab_size: 30522\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: fake_corpus.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 28889 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=196380\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=908\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 28889 sentences.\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: distilbert_tokenizer.model\n",
            "spm_train_main.cc(282) [_status.ok()] Internal: src/trainer_interface.cc(662) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (30522). Please set it to a value <= 26598.\n",
            "Program terminated with an unrecoverable error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model.save(\"distilbert_tf_model\")\n",
        "\n",
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"distilbert_tf_model\")\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "#tflite_model.save()\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"distilbert-qa.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "E5Uv84i_5RLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d804ca26-ca42-4675-89d6-1e032c713b29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x79eabe9b1c90>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x79eab7734610>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x79eab775e9d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x79eabe9d6310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x79eabe93a050>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x79eabe981290>, because it is not built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"spm_train\" 2>/dev/null"
      ],
      "metadata": {
        "id": "9_8iADI8SqQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5265cb-f9d6-4f18-cebf-88a59c4f8760"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/spm_train\n",
            "/content/sentencepiece/build/src/spm_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y cmake build-essential pkg-config libgoogle-perftools-dev\n",
        "!git clone https://github.com/google/sentencepiece.git\n",
        "%cd sentencepiece\n",
        "!mkdir build && cd build && cmake .. && make -j $(nproc) && make install && ldconfig\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "oZFX2ylW3WYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b902fc6-4206-4e26-f9aa-4972566a034d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 2s (148 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libgoogle-perftools-dev is already the newest version (2.9.1-0ubuntu3).\n",
            "pkg-config is already the newest version (0.29.2-1ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libbz2-dev libpkgconf3 libreadline-dev\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "fatal: destination path 'sentencepiece' already exists and is not an empty directory.\n",
            "/content/sentencepiece\n",
            "mkdir: cannot create directory ‘build’: File exists\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spm_train --version"
      ],
      "metadata": {
        "id": "OYDF8dsV4l8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d199db-3bbc-4b15-cd49-0633175a08a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece 0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spm_train \\\n",
        "      --input=fake_corpus.txt \\\n",
        "      --model_prefix=distilbert_tokenizer \\\n",
        "      --vocab_size=26598 \\\n",
        "      --character_coverage=1.0 \\\n",
        "      --model_type=word"
      ],
      "metadata": {
        "id": "jeHbp4-O4oft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55e2c36-5b86-4d65-c5cf-861984da36cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: fake_corpus.txt\n",
            "  input_format: \n",
            "  model_prefix: distilbert_tokenizer\n",
            "  model_type: WORD\n",
            "  vocab_size: 26598\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: fake_corpus.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 28889 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=196380\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=908\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 28889 sentences.\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: distilbert_tokenizer.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: distilbert_tokenizer.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --force-reinstall mediapipe-model-maker==0.2.1.4\n",
        "#!pip install --upgrade  --user mediapipe-model-maker==0.2.1.4\n",
        "!python3 -m mediapipe_model_maker.genai.task_converter \\\n",
        "  --tflite_model distilbert-qa.tflite \\\n",
        "  --tokenizer_model distilbert_tokenizer.model \\\n",
        "  --output_file distilbert_qa.task"
      ],
      "metadata": {
        "id": "aceRhGxH5XSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc908b0-b5f6-4a70-db02-a1fcec205fc8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-11 23:56:52.112007: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-04-11 23:56:52.112061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-04-11 23:56:52.113135: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-11 23:56:52.999620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mediapipe_model_maker/__init__.py\", line 22, in <module>\n",
            "    from mediapipe_model_maker.python.text import text_classifier\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mediapipe_model_maker/python/text/text_classifier/__init__.py\", line 22, in <module>\n",
            "    from mediapipe_model_maker.python.text.text_classifier import text_classifier\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mediapipe_model_maker/python/text/text_classifier/text_classifier.py\", line 22, in <module>\n",
            "    from tensorflow_addons import optimizers as tfa_optimizers\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\", line 23, in <module>\n",
            "    from tensorflow_addons import activations\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\", line 17, in <module>\n",
            "    from tensorflow_addons.activations.gelu import gelu\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\", line 19, in <module>\n",
            "    from tensorflow_addons.utils.types import TensorLike\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\", line 29, in <module>\n",
            "    from keras.src.engine import keras_tensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 3, in <module>\n",
            "    from keras import __internal__\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/__internal__/__init__.py\", line 3, in <module>\n",
            "    from keras.__internal__ import backend\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/__internal__/backend/__init__.py\", line 3, in <module>\n",
            "    from keras.src.backend import _initialize_variables as initialize_variables\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/__init__.py\", line 21, in <module>\n",
            "    from keras.src import applications\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/applications/__init__.py\", line 18, in <module>\n",
            "    from keras.src.applications.convnext import ConvNeXtBase\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/applications/convnext.py\", line 28, in <module>\n",
            "    from keras.src import backend\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/__init__.py\", line 10, in <module>\n",
            "    from keras.src.backend.common.dtypes import result_type\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/__init__.py\", line 2, in <module>\n",
            "    from keras.src.backend.common.dtypes import result_type\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/dtypes.py\", line 5, in <module>\n",
            "    from keras.src.backend.common.variables import standardize_dtype\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\", line 11, in <module>\n",
            "    from keras.src.utils.module_utils import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/__init__.py\", line 53, in <module>\n",
            "    from keras.src.utils.feature_space import FeatureSpace\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/feature_space.py\", line 20, in <module>\n",
            "    from keras.src.engine import base_layer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/base_layer.py\", line 32, in <module>\n",
            "    from keras.src import constraints\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/constraints/__init__.py\", line 4, in <module>\n",
            "    from keras.src.constraints.constraints import Constraint\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/constraints/constraints.py\", line 2, in <module>\n",
            "    from keras.src import ops\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/__init__.py\", line 5, in <module>\n",
            "    from keras.src.backend import cast\n",
            "ImportError: cannot import name 'cast' from partially initialized module 'keras.src.backend' (most likely due to a circular import) (/usr/local/lib/python3.11/dist-packages/keras/src/backend/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip show mediapipe-model-maker"
      ],
      "metadata": {
        "id": "CLu6ITbxyYkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a26bfc-c6dc-4c93-9ca5-b2790288b0ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: mediapipe-model-maker\n",
            "Version: 0.2.1.4\n",
            "Summary: MediaPipe Model Maker is a simple, low-code solution for customizing on-device ML models\n",
            "Home-page: https://github.com/google/mediapipe/tree/master/mediapipe/model_maker\n",
            "Author: The MediaPipe Authors\n",
            "Author-email: mediapipe@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, mediapipe, numpy, opencv-python, tensorflow, tensorflow-addons, tensorflow-datasets, tensorflow-hub, tensorflow-model-optimization, tensorflow-text, tf-models-official\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow text\n",
        "#!pip install tensorflow_hub"
      ],
      "metadata": {
        "id": "_fikOg2Eyjl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb5115a-01a6-4f87-c8b0-3b3c6ae97574"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.15.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement text (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for text\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tensorflow_hub in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow_hub) (4.25.6)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow_hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.15.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (78.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zpnGQA_AzUNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf; print(tf.__version__)"
      ],
      "metadata": {
        "id": "ES12ZzvWzUsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b589d4b-0fa5-4e7f-e208-70f9a3c7f45c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why do we need task bundles?\n",
        "\n",
        "Executing a text generation pipeline in its entirety requires more than merely employing the core transformer model. It requires preparing the input text to align with the model's required format, running the model autoregressively, and sampling during each iteration. Consequently, once the user has converted their model into `tflite` format (comprising the model's graph and parameters), it becomes essential to augment it with additional metadata to ensure successful end-to-end execution of the model.\n",
        "\n",
        "Task bundler offers a practical approach to creating such bundles. The example below illustrates how a task bundle can be generated for a converted Gemma model."
      ],
      "metadata": {
        "id": "5Z6c-cpvqd25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model=\"distilbert-qa.tflite\" # @param {type:\"string\"}\n",
        "tokenizer_model=\"PATH/tokenizer.model\" # @param {type:\"string\"}\n",
        "start_token=\"<bos>\" # @param {type:\"string\"}\n",
        "stop_token=\"<eos>\" # @param {type:\"string\"}\n",
        "output_filename=\"PATH/gemma.task\" # @param {type:\"string\"}\n",
        "enable_bytes_to_unicode_mapping=False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "config = bundler.BundleConfig(\n",
        "    tflite_model=tflite_model,\n",
        "    tokenizer_model=tokenizer_model,\n",
        "    start_token=start_token,\n",
        "    stop_tokens=[stop_token],\n",
        "    output_filename=output_filename,\n",
        "    enable_bytes_to_unicode_mapping=enable_bytes_to_unicode_mapping,\n",
        ")\n",
        "bundler.create_bundle(config)"
      ],
      "metadata": {
        "id": "Cs4KJzSzqb7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "3a984001-d774-4f2e-e6e5-943fa5f19151"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to load tokenizer model from PATH/tokenizer.model. Please ensure you are passing a valid SentencePiece model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/tasks/python/genai/bundler/llm_bundler.py\u001b[0m in \u001b[0;36m_validate_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"PATH/tokenizer.model\": No such file or directory Error #2",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f36d047beceb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0menable_bytes_to_unicode_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_bytes_to_unicode_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbundler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/tasks/python/genai/bundler/llm_bundler.py\u001b[0m in \u001b[0;36mcreate_bundle\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBundleConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;34m\"\"\"Creates a bundle from the given config.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m   \u001b[0m_validate_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/tasks/python/genai/bundler/llm_bundler.py\u001b[0m in \u001b[0;36m_validate_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;34mf\"Failed to load tokenizer model from {config.tokenizer_model}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;34m\"Please ensure you are passing a valid SentencePiece model.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to load tokenizer model from PATH/tokenizer.model. Please ensure you are passing a valid SentencePiece model."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IliBF1FVRx4M",
        "outputId": "fd5ee935-ec71-4c7c-fafb-5beb9d097b48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "\n",
        "* The current task pipeline only supports SentencePiece tokenizer models.\n",
        "* Certain models (e.g., phi-2) use bytes to unicode mapping. Use `enable_bytes_to_unicode_mapping` flag accordingly. Such information, including `start_token` and `stop_tokens` are often provided along with model artifacts.\n",
        "* The generated output bundle file must end with `.task`. If not, `create_bundle` automatically adds the extension. Do not remove or change that."
      ],
      "metadata": {
        "id": "_Unaye8xsfio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the Bundle { display-mode: \"form\" }\n",
        "#@markdown Run this cell to download the generated `.task` file.\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "FnFkpfr1bwtx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}